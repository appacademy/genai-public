"""
LLM API interaction functions
"""

import os
import time
import logging
from typing import Dict, Any

from models.request import LLMRequest
from resilience.retry import retry_with_exponential_backoff
from ollama_client import OllamaClient

# Load environment variables and initialize client
OLLAMA_API_URL = os.getenv("OLLAMA_API_URL")
ollama_client = OllamaClient(OLLAMA_API_URL)


@retry_with_exponential_backoff
def call_llm_api(request: LLMRequest) -> Dict[str, Any]:
    """LLM API call with streaming output"""
    logger = logging.getLogger("llm_app")
    logger.info(
        f"Calling LLM API with model {request.model}",
        extra={"request_id": request.request_id},
    )

    start_time = time.time()

    try:
        # Get streaming response
        response_stream = ollama_client.generate(
            prompt=request.prompt,
            temperature=request.temperature,
            stream=True,
            model=request.model,
        )

        # Collect full response while printing chunks
        full_text = ""
        chunk_counter = 0

        # Print model attribution before the response
        model_attribution = f"\n[Response from {request.model}]\n"
        print(model_attribution, end="", flush=True)

        for chunk in response_stream:
            if "response" in chunk:
                chunk_text = chunk.get("response", "")
                full_text += chunk_text

                # Print the chunk directly to console
                print(chunk_text, end="", flush=True)

                # Show progress indicator for longer responses
                chunk_counter += 1
                if chunk_counter % 20 == 0:
                    # Allow keyboard interrupts by yielding to event loop briefly
                    time.sleep(0.01)

        print("\n")  # End the line after streaming completes

        # Add model attribution to the full text for storage
        full_text = f"{full_text}\n\n[Generated by {request.model}]"

        latency = (time.time() - start_time) * 1000  # Convert to ms
        logger.debug(
            f"LLM API call successful in {latency:.2f}ms",
            extra={"request_id": request.request_id},
        )

        # Estimate token count - Ollama doesn't provide token counts directly
        result = {
            "text": full_text,
            "model_used": request.model,
            "tokens_used": chunk_counter,
            "latency_ms": latency,
            "fallback_used": False,
            "fallback_level": 0,
            "request_id": request.request_id,
        }

        return result

    except Exception as e:
        logger.error(
            f"Error calling LLM API: {str(e)}", extra={"request_id": request.request_id}
        )
        print(f"\nError: {str(e)}")  # Print error to console
        raise
